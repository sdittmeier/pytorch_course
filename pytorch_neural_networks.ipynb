{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Let's set cpu as the device we're using\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network definition for processing MNIST dataset\n",
    "# Initialization and definition of forward pass\n",
    "# The forward pass is the sequence of computations \n",
    "# that are applied to the input data to generate the output.\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()             # 2D image flattened to 1D tensor\n",
    "        self.linear_relu_stack = nn.Sequential( # Sequential container\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the NeuralNetwork class\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: tensor([4])\n"
     ]
    }
   ],
   "source": [
    "# pass input data through the model (with background operations)\n",
    "# don't call model.forward() directly!\n",
    "X = torch.rand(1, 28, 28, device=device)\n",
    "logits = model(X)\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")\n",
    "# Applies the Softmax function to an n-dimensional input Tensor \n",
    "# rescaling them so that the elements of the n-dimensional output \n",
    "# Tensor lie in the range [0,1] and sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n",
      "torch.Size([3, 784])\n",
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# Model Layers\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())\n",
    "\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())\n",
    "\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1841,  0.3871,  0.5317, -0.4767, -0.0439,  0.4800,  0.1283, -0.0593,\n",
      "         -0.7697, -0.0494,  0.2699, -0.1947,  0.6467,  0.1341, -0.7315,  0.0612,\n",
      "          0.0919,  0.1725, -0.2411,  0.0767],\n",
      "        [ 0.4336,  0.0298,  0.5510, -0.0371, -0.1770,  0.3888, -0.1057,  0.2481,\n",
      "         -0.7115, -0.4966, -0.3006,  0.0598,  0.6359, -0.0801, -0.5019,  0.1239,\n",
      "          0.0604,  0.3623, -0.0130,  0.2257],\n",
      "        [ 0.1464,  0.3404,  0.5132, -0.2751, -0.0458,  0.5905,  0.1203, -0.0662,\n",
      "         -0.6861, -0.4931,  0.0819, -0.1058,  0.1905,  0.0613, -0.4318, -0.0037,\n",
      "          0.1319,  0.3603, -0.3865,  0.3146]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1841, 0.3871, 0.5317, 0.0000, 0.0000, 0.4800, 0.1283, 0.0000, 0.0000,\n",
      "         0.0000, 0.2699, 0.0000, 0.6467, 0.1341, 0.0000, 0.0612, 0.0919, 0.1725,\n",
      "         0.0000, 0.0767],\n",
      "        [0.4336, 0.0298, 0.5510, 0.0000, 0.0000, 0.3888, 0.0000, 0.2481, 0.0000,\n",
      "         0.0000, 0.0000, 0.0598, 0.6359, 0.0000, 0.0000, 0.1239, 0.0604, 0.3623,\n",
      "         0.0000, 0.2257],\n",
      "        [0.1464, 0.3404, 0.5132, 0.0000, 0.0000, 0.5905, 0.1203, 0.0000, 0.0000,\n",
      "         0.0000, 0.0819, 0.0000, 0.1905, 0.0613, 0.0000, 0.0000, 0.1319, 0.3603,\n",
      "         0.0000, 0.3146]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([[-0.1418,  0.0911, -0.0047, -0.4088,  0.3273, -0.0013,  0.1207,  0.3697,\n",
      "          0.0728,  0.3245],\n",
      "        [-0.0442,  0.0108, -0.0786, -0.3027,  0.3764,  0.0135,  0.0842,  0.2415,\n",
      "          0.1506,  0.3616],\n",
      "        [-0.0035, -0.1030, -0.0955, -0.3186,  0.3057, -0.2327,  0.0628,  0.2808,\n",
      "          0.1783,  0.2959]], grad_fn=<AddmmBackward0>)\n",
      "pred_probab: tensor([[0.0786, 0.0992, 0.0901, 0.0602, 0.1256, 0.0904, 0.1022, 0.1311, 0.0974,\n",
      "         0.1253],\n",
      "        [0.0865, 0.0914, 0.0836, 0.0668, 0.1317, 0.0916, 0.0984, 0.1151, 0.1051,\n",
      "         0.1298],\n",
      "        [0.0939, 0.0850, 0.0856, 0.0685, 0.1279, 0.0747, 0.1003, 0.1248, 0.1126,\n",
      "         0.1267]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)\n",
    "\n",
    "print(f\"logits: {logits}\")\n",
    "softmax_fn = nn.Softmax(dim=1)\n",
    "pred_probab = softmax_fn(logits)\n",
    "print(f\"pred_probab: {pred_probab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model structure:  NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ") \n",
      "\n",
      "\n",
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([128, 784]) | Values : tensor([[-0.0258, -0.0209, -0.0174,  ...,  0.0249,  0.0165, -0.0068],\n",
      "        [-0.0306, -0.0287, -0.0177,  ..., -0.0336, -0.0170, -0.0127]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([128]) | Values : tensor([0.0193, 0.0070], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([128, 128]) | Values : tensor([[-0.0398, -0.0315,  0.0580,  0.0517, -0.0533, -0.0787,  0.0594, -0.0035,\n",
      "         -0.0874,  0.0714, -0.0564, -0.0076,  0.0731, -0.0260, -0.0338,  0.0883,\n",
      "          0.0132, -0.0410, -0.0360, -0.0830,  0.0428,  0.0531, -0.0674,  0.0678,\n",
      "         -0.0560, -0.0276,  0.0393, -0.0650,  0.0837,  0.0269,  0.0379,  0.0434,\n",
      "         -0.0406, -0.0507,  0.0616,  0.0550,  0.0463, -0.0471,  0.0779, -0.0584,\n",
      "         -0.0841, -0.0151,  0.0411,  0.0074,  0.0085, -0.0695,  0.0046, -0.0817,\n",
      "         -0.0136,  0.0179,  0.0551, -0.0680,  0.0800,  0.0093,  0.0854, -0.0322,\n",
      "          0.0055, -0.0685,  0.0866, -0.0658, -0.0744,  0.0417, -0.0405,  0.0823,\n",
      "          0.0036, -0.0835, -0.0519,  0.0139,  0.0133, -0.0446,  0.0867, -0.0760,\n",
      "         -0.0472, -0.0391,  0.0799, -0.0234,  0.0413, -0.0114,  0.0802, -0.0660,\n",
      "          0.0074, -0.0874, -0.0405, -0.0106,  0.0195, -0.0337, -0.0549, -0.0674,\n",
      "         -0.0685, -0.0396,  0.0654, -0.0289,  0.0244,  0.0189,  0.0558,  0.0733,\n",
      "          0.0851,  0.0376, -0.0174,  0.0415, -0.0022, -0.0802, -0.0457,  0.0088,\n",
      "          0.0320,  0.0827, -0.0146, -0.0584, -0.0648, -0.0020,  0.0613, -0.0852,\n",
      "         -0.0038, -0.0732, -0.0392, -0.0070,  0.0727, -0.0109, -0.0007,  0.0071,\n",
      "         -0.0145,  0.0525,  0.0726,  0.0807,  0.0191,  0.0033,  0.0481, -0.0540],\n",
      "        [ 0.0787,  0.0575, -0.0101, -0.0782, -0.0716,  0.0824,  0.0179, -0.0585,\n",
      "          0.0256, -0.0453, -0.0763, -0.0098,  0.0396,  0.0291, -0.0503,  0.0580,\n",
      "         -0.0861,  0.0329, -0.0808, -0.0237,  0.0467, -0.0720,  0.0358, -0.0687,\n",
      "         -0.0800,  0.0292, -0.0460,  0.0343,  0.0404,  0.0845, -0.0698,  0.0358,\n",
      "         -0.0781, -0.0873, -0.0053, -0.0700, -0.0075, -0.0736,  0.0438,  0.0086,\n",
      "          0.0100, -0.0518, -0.0359, -0.0634,  0.0211,  0.0210, -0.0704,  0.0796,\n",
      "         -0.0842, -0.0489, -0.0869, -0.0443,  0.0637,  0.0587,  0.0047,  0.0690,\n",
      "          0.0686, -0.0264,  0.0180,  0.0388,  0.0172,  0.0361, -0.0184, -0.0450,\n",
      "         -0.0353, -0.0716, -0.0777, -0.0606, -0.0774, -0.0071,  0.0814,  0.0844,\n",
      "          0.0559,  0.0202, -0.0691,  0.0851, -0.0774,  0.0255,  0.0301, -0.0231,\n",
      "         -0.0304,  0.0830,  0.0522,  0.0333,  0.0247,  0.0009, -0.0582, -0.0329,\n",
      "         -0.0840, -0.0300, -0.0857,  0.0246,  0.0083,  0.0448, -0.0100, -0.0285,\n",
      "          0.0055,  0.0338, -0.0373, -0.0086, -0.0556, -0.0692, -0.0427,  0.0192,\n",
      "          0.0447, -0.0855, -0.0412, -0.0261, -0.0107,  0.0691,  0.0466, -0.0253,\n",
      "          0.0286, -0.0512, -0.0173, -0.0635, -0.0830, -0.0231, -0.0884,  0.0702,\n",
      "         -0.0794,  0.0141,  0.0185,  0.0172, -0.0795,  0.0189, -0.0753,  0.0495]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([128]) | Values : tensor([-0.0459,  0.0792], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 128]) | Values : tensor([[-0.0251, -0.0678, -0.0106,  0.0640,  0.0308, -0.0735,  0.0799,  0.0348,\n",
      "          0.0280, -0.0176,  0.0860, -0.0544, -0.0303, -0.0240,  0.0837, -0.0190,\n",
      "          0.0563,  0.0142,  0.0452,  0.0050,  0.0007, -0.0461, -0.0696, -0.0365,\n",
      "          0.0533, -0.0477,  0.0022, -0.0800, -0.0146, -0.0673,  0.0307,  0.0088,\n",
      "         -0.0639, -0.0155, -0.0366,  0.0379, -0.0614, -0.0155, -0.0497,  0.0654,\n",
      "         -0.0266, -0.0504, -0.0854, -0.0527, -0.0702,  0.0853, -0.0740,  0.0379,\n",
      "          0.0807, -0.0291,  0.0481,  0.0701,  0.0096, -0.0163,  0.0839, -0.0824,\n",
      "         -0.0845,  0.0349,  0.0343,  0.0698,  0.0465,  0.0879, -0.0440, -0.0357,\n",
      "         -0.0817,  0.0600, -0.0805,  0.0465, -0.0800,  0.0584,  0.0602,  0.0024,\n",
      "          0.0243,  0.0563,  0.0522,  0.0750,  0.0137,  0.0280,  0.0563, -0.0659,\n",
      "         -0.0502, -0.0794, -0.0223,  0.0626, -0.0338, -0.0381, -0.0272, -0.0384,\n",
      "         -0.0086, -0.0107,  0.0852,  0.0713,  0.0009, -0.0741, -0.0793, -0.0573,\n",
      "          0.0062, -0.0851, -0.0456, -0.0559,  0.0753,  0.0367,  0.0133, -0.0429,\n",
      "          0.0551, -0.0833,  0.0537,  0.0140,  0.0318, -0.0584,  0.0854, -0.0456,\n",
      "         -0.0817, -0.0195, -0.0855,  0.0339,  0.0722,  0.0848,  0.0695,  0.0752,\n",
      "          0.0751, -0.0137, -0.0123,  0.0320,  0.0154,  0.0805,  0.0224, -0.0793],\n",
      "        [-0.0226, -0.0248,  0.0172,  0.0631, -0.0327, -0.0452,  0.0067, -0.0735,\n",
      "          0.0796, -0.0027,  0.0160, -0.0451,  0.0068,  0.0805, -0.0130, -0.0354,\n",
      "          0.0883,  0.0833,  0.0412, -0.0249,  0.0786, -0.0842,  0.0595, -0.0168,\n",
      "         -0.0835,  0.0533, -0.0370, -0.0627, -0.0332, -0.0243,  0.0627,  0.0111,\n",
      "         -0.0465, -0.0043, -0.0157, -0.0719, -0.0008, -0.0742,  0.0863,  0.0341,\n",
      "          0.0778,  0.0222,  0.0193,  0.0771, -0.0655,  0.0835, -0.0505,  0.0569,\n",
      "          0.0067, -0.0174,  0.0203, -0.0661,  0.0238, -0.0147, -0.0008,  0.0223,\n",
      "          0.0153, -0.0395, -0.0303, -0.0582, -0.0768,  0.0482, -0.0710, -0.0683,\n",
      "          0.0353,  0.0003, -0.0117, -0.0601, -0.0096,  0.0214,  0.0444,  0.0050,\n",
      "         -0.0077, -0.0608, -0.0784,  0.0643,  0.0618, -0.0260,  0.0559, -0.0565,\n",
      "         -0.0080,  0.0155,  0.0647,  0.0714,  0.0056,  0.0860, -0.0673,  0.0144,\n",
      "         -0.0134,  0.0421,  0.0699,  0.0375, -0.0813, -0.0717,  0.0861, -0.0709,\n",
      "         -0.0158,  0.0154,  0.0457,  0.0849, -0.0107, -0.0042, -0.0116,  0.0052,\n",
      "          0.0290,  0.0036,  0.0781, -0.0403,  0.0022,  0.0147,  0.0377,  0.0085,\n",
      "          0.0340, -0.0159,  0.0685,  0.0304, -0.0226,  0.0861, -0.0569, -0.0585,\n",
      "          0.0769,  0.0792,  0.0828,  0.0068, -0.0250, -0.0039,  0.0509,  0.0862]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([ 0.0707, -0.0715], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Model Parameters\n",
    "\n",
    "print(\"Model structure: \", model, \"\\n\\n\")\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
