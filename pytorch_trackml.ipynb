{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/anaconda3/envs/gnn4itk/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import trackml.dataset\n",
    "from trackml.utils import add_momentum_quantities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:7\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "event_path = \"/home/atlas/dittmeier/public_html/pytorch/trackml_data/trainset/event000021000\"\n",
    "\n",
    "particles, hits, cells, truth = trackml.dataset.load_event(\n",
    "    event_path, parts=[\"particles\", \"hits\", \"cells\", \"truth\"]\n",
    ")\n",
    "\n",
    "print(truth.head())\n",
    "print(truth[\"particle_id\"].value_counts())\n",
    "\n",
    "#print(count.item())\n",
    "\n",
    "\n",
    "# Create histograms for particles\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(particles.columns):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.hist(particles[column], bins=50)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create histograms for hits\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(hits.columns):\n",
    "    plt.subplot(2, 4, i+1)\n",
    "    plt.hist(hits[column], bins=50)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create histograms for cells\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(cells.columns):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist(cells[column], bins=50)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create histograms for truth\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(truth.columns):\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.hist(truth[column], bins=50)\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset, with a label for each hit, if hits have same particle_id set to 1, else -1\n",
    "# and an option to apply a cut on pT on the dataset\n",
    "\n",
    "class TrackMLDataset(Dataset):\n",
    "    def __init__(self, event_path, nevents=10, cuts=None):\n",
    "        self.event_path = event_path\n",
    "        self.nevents = nevents\n",
    "        self.event_ids = []\n",
    "        self.particles = []\n",
    "        self.hits = []\n",
    "        self.cells = []\n",
    "        self.truth = []\n",
    "        self.labels = []\n",
    "\n",
    "        for event_id, hits, cells, particles, truth in trackml.dataset.load_dataset(event_path, nevents=nevents):\n",
    "            particles = add_momentum_quantities(particles)\n",
    "            print(event_id)\n",
    "            if cuts is not None:\n",
    "                print(f\"Applying cuts: {cuts}\")\n",
    "                particles = particles[particles.pt >= cuts[\"pt\"]].reset_index(drop=True)\n",
    "                truth = truth[truth.particle_id.isin(particles.particle_id)].reset_index(drop=True)\n",
    "                hits  = hits[hits.hit_id.isin(truth.hit_id)].reset_index(drop=True)\n",
    "                cells = cells[cells.hit_id.isin(hits.hit_id)].reset_index(drop=True)\n",
    "                \n",
    "            self.event_ids.append(event_id)\n",
    "            self.particles.append(particles)\n",
    "            self.hits.append(hits)\n",
    "            self.cells.append(cells)\n",
    "            self.truth.append(truth)\n",
    "            # create a label for a matrix of shape (n_hits, n_hits), if hits have same particle_id set to 1, else -1\n",
    "            # we can be more sophisticated here, but for now this is enough\n",
    "            labels = np.full((len(hits), len(hits)), -1, dtype=np.int8)    # this is not memory friendly!!!\n",
    "            for particle_id, nhits in zip(particles.particle_id, particles.nhits):\n",
    "                particle_hits = truth[truth.particle_id == particle_id].index\n",
    "                #meshgrid = np.meshgrid(particle_hits, particle_hits)\n",
    "                #print(f\"particle_id: {particle_id}, nhits = {nhits}, particle_hits = {particle_hits}, meshgrid = {meshgrid}\")\n",
    "                #labels[meshgrid] = 1\n",
    "                for match in particle_hits:\n",
    "                    for match2 in particle_hits:\n",
    "                        labels[match, match2] = 1\n",
    "#            print(labels.size)\n",
    "            #if np.array_equal(labels, labels2):\n",
    "            #    print(\"Labels are equal\")\n",
    "            #count_1 = np.count_nonzero(labels == 1)\n",
    "            #print(f\"Count of 1 in labels: {count_1}\")\n",
    "            #count_2= np.count_nonzero(labels2 == 1)\n",
    "            #print(f\"Count of 1 in labels2: {count_2}\")\n",
    "            #labels = np.full((1,1),0)\n",
    "            self.labels.append(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.event_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        event_id = self.event_ids[idx]\n",
    "        particles = self.particles[idx]\n",
    "        hits = self.hits[idx]\n",
    "        cells = self.cells[idx]\n",
    "        truth = self.truth[idx]\n",
    "        labels = self.labels[idx]\n",
    "        return event_id, torch.tensor(particles.values, dtype=torch.float32), torch.tensor(hits.values, dtype=torch.float32), torch.tensor(cells.values, dtype=torch.float32), torch.tensor(truth.values, dtype=torch.float32), torch.from_numpy(labels).type(torch.int8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Trainset\n",
      "21000\n",
      "Applying cuts: {'pt': 1}\n",
      "21001\n",
      "Applying cuts: {'pt': 1}\n",
      "21002\n",
      "Applying cuts: {'pt': 1}\n",
      "21003\n",
      "Applying cuts: {'pt': 1}\n",
      "21004\n",
      "Applying cuts: {'pt': 1}\n",
      "21005\n",
      "Applying cuts: {'pt': 1}\n",
      "21006\n",
      "Applying cuts: {'pt': 1}\n",
      "21007\n",
      "Applying cuts: {'pt': 1}\n",
      "21008\n",
      "Applying cuts: {'pt': 1}\n",
      "21009\n",
      "Applying cuts: {'pt': 1}\n",
      "21010\n",
      "Applying cuts: {'pt': 1}\n",
      "21011\n",
      "Applying cuts: {'pt': 1}\n",
      "21012\n",
      "Applying cuts: {'pt': 1}\n",
      "21013\n",
      "Applying cuts: {'pt': 1}\n",
      "21014\n",
      "Applying cuts: {'pt': 1}\n",
      "21015\n",
      "Applying cuts: {'pt': 1}\n",
      "21016\n",
      "Applying cuts: {'pt': 1}\n",
      "21017\n",
      "Applying cuts: {'pt': 1}\n",
      "21018\n",
      "Applying cuts: {'pt': 1}\n",
      "21019\n",
      "Applying cuts: {'pt': 1}\n",
      "21020\n",
      "Applying cuts: {'pt': 1}\n",
      "21021\n",
      "Applying cuts: {'pt': 1}\n",
      "21022\n",
      "Applying cuts: {'pt': 1}\n",
      "21023\n",
      "Applying cuts: {'pt': 1}\n",
      "21024\n",
      "Applying cuts: {'pt': 1}\n",
      "21025\n",
      "Applying cuts: {'pt': 1}\n",
      "21026\n",
      "Applying cuts: {'pt': 1}\n",
      "21027\n",
      "Applying cuts: {'pt': 1}\n",
      "21028\n",
      "Applying cuts: {'pt': 1}\n",
      "21029\n",
      "Applying cuts: {'pt': 1}\n",
      "21030\n",
      "Applying cuts: {'pt': 1}\n",
      "21031\n",
      "Applying cuts: {'pt': 1}\n",
      "21032\n",
      "Applying cuts: {'pt': 1}\n",
      "21033\n",
      "Applying cuts: {'pt': 1}\n",
      "21034\n",
      "Applying cuts: {'pt': 1}\n",
      "21035\n",
      "Applying cuts: {'pt': 1}\n",
      "21036\n",
      "Applying cuts: {'pt': 1}\n",
      "21037\n",
      "Applying cuts: {'pt': 1}\n",
      "21038\n",
      "Applying cuts: {'pt': 1}\n",
      "21039\n",
      "Applying cuts: {'pt': 1}\n",
      "21040\n",
      "Applying cuts: {'pt': 1}\n",
      "21041\n",
      "Applying cuts: {'pt': 1}\n",
      "21042\n",
      "Applying cuts: {'pt': 1}\n",
      "21043\n",
      "Applying cuts: {'pt': 1}\n",
      "21044\n",
      "Applying cuts: {'pt': 1}\n",
      "21045\n",
      "Applying cuts: {'pt': 1}\n",
      "21046\n",
      "Applying cuts: {'pt': 1}\n",
      "21047\n",
      "Applying cuts: {'pt': 1}\n",
      "21048\n",
      "Applying cuts: {'pt': 1}\n",
      "21049\n",
      "Applying cuts: {'pt': 1}\n",
      "Loading Valset\n",
      "21090\n",
      "Applying cuts: {'pt': 1}\n",
      "21091\n",
      "Applying cuts: {'pt': 1}\n",
      "21092\n",
      "Applying cuts: {'pt': 1}\n",
      "21093\n",
      "Applying cuts: {'pt': 1}\n",
      "21094\n",
      "Applying cuts: {'pt': 1}\n",
      "21095\n",
      "Applying cuts: {'pt': 1}\n",
      "21096\n",
      "Applying cuts: {'pt': 1}\n",
      "21097\n",
      "Applying cuts: {'pt': 1}\n",
      "21098\n",
      "Applying cuts: {'pt': 1}\n",
      "21099\n",
      "Applying cuts: {'pt': 1}\n",
      "Loading Testset\n",
      "21080\n",
      "Applying cuts: {'pt': 1}\n",
      "21081\n",
      "Applying cuts: {'pt': 1}\n",
      "21082\n",
      "Applying cuts: {'pt': 1}\n",
      "21083\n",
      "Applying cuts: {'pt': 1}\n",
      "21084\n",
      "Applying cuts: {'pt': 1}\n",
      "21085\n",
      "Applying cuts: {'pt': 1}\n",
      "21086\n",
      "Applying cuts: {'pt': 1}\n",
      "21087\n",
      "Applying cuts: {'pt': 1}\n",
      "21088\n",
      "Applying cuts: {'pt': 1}\n",
      "21089\n",
      "Applying cuts: {'pt': 1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Trainset\")\n",
    "event_path = \"/home/atlas/dittmeier/public_html/pytorch/trackml_data/trainset\"\n",
    "training_data = TrackMLDataset(event_path, nevents=40, cuts={\"pt\": 1})\n",
    "print(\"Loading Valset\")\n",
    "event_path = \"/home/atlas/dittmeier/public_html/pytorch/trackml_data/valset\"\n",
    "val_data = TrackMLDataset(event_path, cuts={\"pt\": 1})\n",
    "print(\"Loading Testset\")\n",
    "event_path = \"/home/atlas/dittmeier/public_html/pytorch/trackml_data/testset\"\n",
    "test_data = TrackMLDataset(event_path, cuts={\"pt\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id shape: torch.Size([1]); event_id data type: torch.int64\n",
      "particles shape: torch.Size([1, 905, 14]); particles data type: torch.float32\n",
      "hits shape: torch.Size([1, 9685, 7]); hits data type: torch.float32\n",
      "cells shape: torch.Size([1, 29479, 4]); cells data type: torch.float32\n",
      "truth shape: torch.Size([1, 9685, 9]); truth data type: torch.float32\n",
      "event_id: 21041\n",
      "hit coordinates: tensor([[  -74.6183,    -8.1546, -1502.5000],\n",
      "        [  -54.1529,   -27.5452, -1502.0000],\n",
      "        [ -111.3620,   -51.9861, -1502.0000],\n",
      "        ...,\n",
      "        [ -867.1360,   488.6900,  2952.5000],\n",
      "        [ -904.3210,   298.8160,  2952.5000],\n",
      "        [ -754.5060,   131.1770,  2947.5000]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "event_id, particles, hits, cells, truth, labels = next(iter(train_dataloader))\n",
    "#event_id, particles, hits, cells, truth = next(iter(train_dataloader))\n",
    "print(f\"event_id shape: {event_id.size()}; event_id data type: {event_id.dtype}\")\n",
    "print(f\"particles shape: {particles.size()}; particles data type: {particles.dtype}\")\n",
    "print(f\"hits shape: {hits.size()}; hits data type: {hits.dtype}\")\n",
    "print(f\"cells shape: {cells.size()}; cells data type: {cells.dtype}\")\n",
    "print(f\"truth shape: {truth.size()}; truth data type: {truth.dtype}\")\n",
    "#print(f\"labels shape: {labels.size()}; labels data type: {labels.dtype}\")\n",
    "\n",
    "print(f\"event_id: {event_id.squeeze()}\")\n",
    "print(f\"hit coordinates: {hits.squeeze()[:,1:4]}\")    # x, y, z\n",
    "#print(f\"labels: {labels.squeeze()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()       \n",
    "        self.linear_relu_stack = nn.Sequential( # Sequential container\n",
    "            nn.Linear(3, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 8),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear_relu_stack(x)\n",
    "        return y\n",
    "\n",
    "# sends the model to GPU\n",
    "model = NeuralNetwork().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = batch_size\n",
    "epochs = 1\n",
    "margin = 1.0\n",
    "\n",
    "loss_fn = nn.HingeEmbeddingLoss(margin=margin)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test run\n",
    "model.eval()\n",
    "result = model(hits.squeeze()[:,1:4].to(device))\n",
    "print(result.size())\n",
    "# calculate distances between all hits\n",
    "distances = torch.cdist(result, result)\n",
    "print(distances.size())\n",
    "print(distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to do a knn from scikit\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "neigh = NearestNeighbors(n_neighbors=20)\n",
    "neigh.fit(result.detach().numpy())\n",
    "distances, indices = neigh.kneighbors(result.detach().numpy())\n",
    "#print(indices)\n",
    "positives = 0\n",
    "negatives = 0\n",
    "\n",
    "count_1 = torch.sum(labels == 1).item()\n",
    "print(f\"Count of 1 in labels: {count_1}\")\n",
    "\n",
    "for ind in indices:\n",
    "    # we can just check with labels, if knn and label = 1 --> match, -1 --> bad\n",
    "    #print(ind)\n",
    "    positives += np.sum((labels.squeeze()[ind[0],ind[1:]] == 1).numpy())\n",
    "    negatives += np.sum((labels.squeeze()[ind[0],ind[1:]] == -1).numpy())\n",
    "    #print(f\"positives: {positives}; negatives: {negatives}\")\n",
    "    \n",
    "print(f\"positives: {positives}; negatives: {negatives}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over our optimization code\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    #print(f\"size = {size}\")\n",
    "    for batch, [event_id, particles, hits, cells, truth, labels] in enumerate(dataloader):\n",
    "        # Send data to GPU\n",
    "        particles, hits, cells, truth, labels = to_device([particles, hits, cells, truth, labels], device)\n",
    "        \n",
    "        embedding = model(hits.squeeze()[:,1:4])\n",
    "        distances = torch.cdist(embedding, embedding)        \n",
    "\n",
    "        # creating labels here is time consuming.\n",
    "        \n",
    "        #labels = torch.ones((hits.squeeze().size(dim=0), hits.squeeze().size(dim=0)), dtype=torch.int8)*(-1)    # this is not memory friendly!!!\n",
    "        #labels = labels.to(device)\n",
    "\n",
    "        #for particle_id, nhits in zip (particles.squeeze()[:,0], particles.squeeze()[:,9]):\n",
    "        #    truth = truth.squeeze()\n",
    "        #    mask = truth[:,1] == particle_id\n",
    "            #print(mask)\n",
    "            #count_trues = torch.sum(mask).item()\n",
    "            #print(f\"Count of True values in mask: {count_trues}, compare to nhits :{nhits}\")\n",
    "        #    true_positions = mask.nonzero().flatten()\n",
    "            #print(true_positions)\n",
    "        #    for match in true_positions:\n",
    "        #       for match2 in true_positions:\n",
    "        #           labels[match, match2] = 1\n",
    "        #count_ones = torch.sum(labels == 1).item()\n",
    "        #print(f\"Count of ones in labels: {count_ones}\")        \n",
    "        loss = loss_fn(distances, labels)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 1 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(event_id)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        # Clear GPU memory\n",
    "        del event_id, particles, hits, cells, truth, embedding, distances, labels, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model's performance against the test dataset\n",
    "def test_loop(dataloader, model, loss_fn, device):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, sum_of_residuals = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for event_id, particles, hits, cells, truth, labels  in dataloader:\n",
    "            # Send data to GPU\n",
    "            particles, hits, cells, truth, labels = to_device([particles, hits, cells, truth, labels], device)\n",
    "            \n",
    "            embedding = model(hits.squeeze()[:,1:4])\n",
    "            distances = torch.cdist(embedding, embedding)\n",
    "\n",
    "            #labels = torch.ones((hits.squeeze().size(dim=0), hits.squeeze().size(dim=0)), dtype=torch.int8)*(-1)    # this is not memory friendly!!!\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            #for particle_id, nhits in zip (particles.squeeze()[:,0], particles.squeeze()[:,9]):\n",
    "            #    truth = truth.squeeze()\n",
    "            #    mask = truth[:,1] == particle_id\n",
    "            #    #print(mask)\n",
    "            #    #count_trues = torch.sum(mask).item()\n",
    "            #    #print(f\"Count of True values in mask: {count_trues}, compare to nhits :{nhits}\")\n",
    "            #    true_positions = mask.nonzero().flatten()\n",
    "            #    #print(true_positions)\n",
    "            #    for match in true_positions:\n",
    "            #        for match2 in true_positions:\n",
    "            #            labels[match, match2] = 1\n",
    "            ##count_ones = torch.sum(labels == 1).item()\n",
    "            ##print(f\"Count of ones in labels: {count_ones}\")\n",
    "\n",
    "\n",
    "            test_loss += loss_fn(distances, labels).item()\n",
    "            del event_id, particles, hits, cells, truth, embedding, distances, labels\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.114261  [    1/   50]\n",
      "loss: 0.132981  [    2/   50]\n",
      "loss: 0.125559  [    3/   50]\n",
      "loss: 0.099772  [    4/   50]\n",
      "loss: 0.132228  [    5/   50]\n",
      "loss: 0.092991  [    6/   50]\n",
      "loss: 0.113123  [    7/   50]\n",
      "loss: 0.077410  [    8/   50]\n",
      "loss: 0.089579  [    9/   50]\n",
      "loss: 0.076422  [   10/   50]\n",
      "loss: 0.066747  [   11/   50]\n",
      "loss: 0.074070  [   12/   50]\n",
      "loss: 0.057488  [   13/   50]\n",
      "loss: 0.069394  [   14/   50]\n",
      "loss: 0.066744  [   15/   50]\n",
      "loss: 0.062766  [   16/   50]\n",
      "loss: 0.046690  [   17/   50]\n",
      "loss: 0.044129  [   18/   50]\n",
      "loss: 0.038634  [   19/   50]\n",
      "loss: 0.035923  [   20/   50]\n",
      "loss: 0.036586  [   21/   50]\n",
      "loss: 0.030957  [   22/   50]\n",
      "loss: 0.026448  [   23/   50]\n",
      "loss: 0.051384  [   24/   50]\n",
      "loss: 0.026868  [   25/   50]\n",
      "loss: 0.034084  [   26/   50]\n",
      "loss: 0.024167  [   27/   50]\n",
      "loss: 0.025820  [   28/   50]\n",
      "loss: 0.024173  [   29/   50]\n",
      "loss: 0.024144  [   30/   50]\n",
      "loss: 0.026942  [   31/   50]\n",
      "loss: 0.019273  [   32/   50]\n",
      "loss: 0.021355  [   33/   50]\n",
      "loss: 0.026515  [   34/   50]\n",
      "loss: 0.019881  [   35/   50]\n",
      "loss: 0.025760  [   36/   50]\n",
      "loss: 0.019132  [   37/   50]\n",
      "loss: 0.017006  [   38/   50]\n",
      "loss: 0.016081  [   39/   50]\n",
      "loss: 0.013286  [   40/   50]\n",
      "loss: 0.013128  [   41/   50]\n",
      "loss: 0.015281  [   42/   50]\n",
      "loss: 0.013611  [   43/   50]\n",
      "loss: 0.013861  [   44/   50]\n",
      "loss: 0.014113  [   45/   50]\n",
      "loss: 0.015111  [   46/   50]\n",
      "loss: 0.012356  [   47/   50]\n",
      "loss: 0.014477  [   48/   50]\n",
      "loss: 0.010984  [   49/   50]\n",
      "loss: 0.014411  [   50/   50]\n",
      "Test Error: \n",
      " Avg loss: 0.011849 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "val_loss = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer, device)\n",
    "    val_loss.append(test_loop(val_dataloader, model, loss_fn, device))\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, param in model.named_parameters():\n",
    "#    print(f\"Parameter name: {name}, Size: {param.size()}, Values: {param}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Assuming you have the `test_loss` variable containing the loss values for each epoch\n",
    "epochs = range(1, len(test_loss) + 1)\n",
    "plt.plot(epochs, test_loss, 'b', label='Test Loss')\n",
    "plt.title('Test Loss vs Epoch')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.random.uniform(-1,1,size=(50, Ndims))\n",
    "model.eval()\n",
    "output = model(torch.tensor(array).float())\n",
    "function_without_noise = np.zeros(50)\n",
    "for i, x in enumerate(array):\n",
    "    function_without_noise[i] = np.sum(coefficients*x)\n",
    "#print(function_without_noise)\n",
    "for i in range(5):\n",
    "    print(array[i], output[i])\n",
    "plt.plot(array[:,0], output.detach().numpy(), 'ro', label='Model Output')\n",
    "plt.plot(array[:,0], function_without_noise, 'b*', label='Function without Noise')\n",
    "plt.xlabel('Feature0')\n",
    "plt.ylabel('Output')\n",
    "plt.title('Model Output vs Function without Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn4itk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
