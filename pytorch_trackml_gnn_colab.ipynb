{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data \n",
    "!mkdir graph_data\n",
    "!wget https://www.physi.uni-heidelberg.de/~dittmeier/pytorch/graph_data/graphs_pT2GeV.zip\n",
    "#wget https://www.physi.uni-heidelberg.de/~dittmeier/pytorch/graph_data/graphs_no_pTCut.zip # uncomment this line to download the data without pT cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip graphs_pT2GeV.zip -d graph_data\n",
    "#!unzip graphs_no_pTCut.zip -d graph_data   # uncomment this line to unzip the data without pT cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'graph_data/pTge2GeV'\n",
    "#data_dir = 'graph_data/nopTCut/' # uncomment this line to use the data without pT cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install required packages\n",
    "import os\n",
    "import torch\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "#ensure that the PyTorch and the PyG are the same version\n",
    "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n",
    "!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n",
    "\n",
    "# Helper function for visualization.\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Define the path to the PyG file\n",
    "file_path = f'{data_dir}/trainset/event000021000.pyg'\n",
    "\n",
    "# Load the PyG file\n",
    "data = torch.load(file_path)\n",
    "\n",
    "# Print the properties of the PyG file\n",
    "print(data)\n",
    "print(data.num_nodes)\n",
    "print(data.num_edges)\n",
    "print(data.edge_index)\n",
    "print(data.y)\n",
    "print(data.truth_map)\n",
    "print(data.track_edges)\n",
    "print(data.x)\n",
    "print(data.r)\n",
    "print(data.phi)\n",
    "print(data.z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.version_utils import get_pyg_data_keys\n",
    "from utils import (\n",
    "    load_datafiles_in_dir,\n",
    "    handle_weighting,\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "\n",
    "class GraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    The custom default GNN dataset to load graphs off the disk\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dir,\n",
    "        data_name=None,\n",
    "        num_events=None,\n",
    "        stage=\"fit\",\n",
    "        hparams=None,\n",
    "        transform=None,\n",
    "        pre_transform=None,\n",
    "        pre_filter=None,\n",
    "        preprocess=True,\n",
    "    ):\n",
    "        if hparams is None:\n",
    "            hparams = {}\n",
    "        super().__init__(input_dir, transform, pre_transform, pre_filter)\n",
    "\n",
    "        self.input_dir = input_dir\n",
    "        self.data_name = data_name\n",
    "        self.hparams = hparams\n",
    "        self.num_events = num_events\n",
    "        self.stage = stage\n",
    "        self.preprocess = preprocess\n",
    "        self.transform = transform\n",
    "\n",
    "        self.input_paths = load_datafiles_in_dir(\n",
    "            self.input_dir, self.data_name, self.num_events\n",
    "        )\n",
    "        self.input_paths.sort()  # We sort here for reproducibility\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.input_paths)\n",
    "\n",
    "    def get(self, idx):\n",
    "        event_path = self.input_paths[idx]\n",
    "        event = torch.load(event_path, map_location=torch.device(\"cpu\"))\n",
    "        # convert DataBatch to Data instance because some transformations don't work on DataBatch\n",
    "        event = Data(**event.to_dict())\n",
    "        if not self.preprocess:\n",
    "            return event\n",
    "        event = self.preprocess_event(event)\n",
    "        # do pyg transformation if a torch_geometric.transforms instance is given\n",
    "        if self.transform is not None:\n",
    "            event = self.transform(event)\n",
    "\n",
    "        # return (event, event_path) if self.stage == \"predict\" else event\n",
    "        return event\n",
    "    \n",
    "    def preprocess_event(self, event):\n",
    "        \"\"\"\n",
    "        Process event before it is used in training and validation loops\n",
    "        \"\"\"\n",
    "        event = self.construct_weighting(event)\n",
    "        event = self.scale_features(event)\n",
    "        return event\n",
    "\n",
    "    def construct_weighting(self, event):\n",
    "        \"\"\"\n",
    "        Construct the weighting for the event\n",
    "        \"\"\"\n",
    "\n",
    "        assert event.y.shape[0] == event.edge_index.shape[1], (\n",
    "            f\"Input graph has {event.edge_index.shape[1]} edges, but\"\n",
    "            f\" {event.y.shape[0]} truth labels\"\n",
    "        )\n",
    "\n",
    "        if self.hparams is not None and \"weighting\" in self.hparams.keys():\n",
    "            assert isinstance(self.hparams[\"weighting\"], list) & isinstance(\n",
    "                self.hparams[\"weighting\"][0], dict\n",
    "            ), \"Weighting must be a list of dictionaries\"\n",
    "            event.weights = handle_weighting(event, self.hparams[\"weighting\"])\n",
    "        else:\n",
    "            event.weights = torch.ones_like(event.y, dtype=torch.float32)\n",
    "\n",
    "        return event\n",
    "\n",
    "\n",
    "    def scale_features(self, event):\n",
    "        \"\"\"\n",
    "        Handle feature scaling for the event\n",
    "        \"\"\"\n",
    "\n",
    "        if (\n",
    "            self.hparams is not None\n",
    "            and \"node_scales\" in self.hparams.keys()\n",
    "            and \"node_features\" in self.hparams.keys()\n",
    "        ):\n",
    "            assert isinstance(\n",
    "                self.hparams[\"node_scales\"], list\n",
    "            ), \"Feature scaling must be a list of ints or floats\"\n",
    "            for i, feature in enumerate(self.hparams[\"node_features\"]):\n",
    "                assert feature in get_pyg_data_keys(\n",
    "                    event\n",
    "                ), f\"Feature {feature} not found in event\"\n",
    "                event[feature] = event[feature] / self.hparams[\"node_scales\"][i]\n",
    "\n",
    "        return event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting for the different classes\n",
    "# false: edges that we want to get rid of\n",
    "weighting_false = {\n",
    "    \"weight\": # choose a weight (floating point value >= 0)\n",
    "    \"conditions\": {\n",
    "        \"y\": False\n",
    "    }\n",
    "}\n",
    "\n",
    "# bckg: edges that are not of interest\n",
    "weighting_bckg = {\n",
    "    \"weight\": # choose a weight (floating point value >= 0)\n",
    "    \"conditions\": {\n",
    "        \"y\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "# target: edges that are of interest, we want to keep them\n",
    "weighting_target = {\n",
    "    \"weight\": # choose a weight (floating point value >= 0)\n",
    "    \"conditions\": {\n",
    "        \"y\": True,\n",
    "        \"pt\": [1., float('inf')],\n",
    "        \"nhits\": [3, float('inf')]\n",
    "    }\n",
    "}\n",
    "\n",
    "# hyperparameters for the dataset\n",
    "parameters = {\n",
    "    \"node_features\": [\"r\",  \"phi\", \"z\"],\n",
    "    \"node_scales\": [1000, 3.14,  1000],\n",
    "    \"weighting\": [weighting_false,\n",
    "                  weighting_bckg,\n",
    "                  weighting_target], # list of dictionaries\n",
    "}\n",
    "\n",
    "\n",
    "trainset = GraphDataset(f\"{data_dir}/trainset\", hparams=parameters)\n",
    "valset = GraphDataset(f\"{data_dir}/valset\", hparams=parameters)\n",
    "testset = GraphDataset(f\"{data_dir}/testset\", hparams=parameters)\n",
    "print(\"Number of samples in trainset:\", len(trainset))\n",
    "print(\"Number of samples in valset:\", len(valset))\n",
    "print(\"Number of samples in testset:\", len(testset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Set batch size and number of workers\n",
    "batch_size = 1\n",
    "num_workers = 1\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "for test_event in train_loader:\n",
    "    print(test_event.r)\n",
    "    print(test_event.y.shape)\n",
    "    print(test_event.weights.shape)\n",
    "    print(test_event.weights)\n",
    "    num_zero_weights = (test_event.weights == 0).sum().item()   \n",
    "    print(\"Number of 0 weights:\", num_zero_weights)\n",
    "    num_point1_weights = (test_event.weights == 0.1).sum().item()\n",
    "    print(\"Number of 0.1 weights:\", num_point1_weights)\n",
    "    num_one_weights = (test_event.weights == 1).sum().item()\n",
    "    print(\"Number of 1 weights:\", num_one_weights)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_gnn.html\n",
    "\n",
    "class InteractionConv(MessagePassing):\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_dim,\n",
    "        aggr=\"add\",\n",
    "        *,\n",
    "        aggr_kwargs={},\n",
    "        flow: str = \"source_to_target\",\n",
    "        node_dim: int = -2,\n",
    "        decomposed_layers: int = 1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            aggr,\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "            flow=flow,\n",
    "            node_dim=node_dim,\n",
    "            decomposed_layers=decomposed_layers,\n",
    "            **kwargs\n",
    "        )\n",
    "        \n",
    "        # example network for node updates; can be replaced with any other network. Experiment here!\n",
    "        self.node_network = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # example network for edge updates; can be replaced with any other network. Experiment here!\n",
    "        self.edge_network = nn.Sequential(\n",
    "            nn.Linear(3*hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def message(self,e):\n",
    "        # constructs messages for each edge; e as given to propagate\n",
    "        return e\n",
    "\n",
    "    def aggregate(\n",
    "        self,\n",
    "        inputs: torch.Tensor,\n",
    "        index: torch.Tensor,\n",
    "        edge_index,\n",
    "        x,\n",
    "        ptr=None,\n",
    "        dim_size=None,\n",
    "    ) -> torch.Tensor:\n",
    "        # takes output from message as first argument (inputs = e), and can take any other data passed to propagate\n",
    "        # so the edge data is creating a message for source and destination nodes\n",
    "        src_message = self.aggr_module(inputs, edge_index[1], dim_size=x.size(0))\n",
    "        dst_message = self.aggr_module(inputs, edge_index[0], dim_size=x.size(0))\n",
    "        out = src_message + dst_message\n",
    "        return out\n",
    "\n",
    "    def update(self, inputs: torch.Tensor, x) -> torch.Tensor:\n",
    "        # takes the aggregated messages and the node data and updates the node data\n",
    "        x_in = torch.cat([x, inputs], dim=1)\n",
    "        out = self.node_network(x_in)\n",
    "        return out\n",
    "\n",
    "    def edge_update(self, edge_index, x, e) -> torch.Tensor:\n",
    "        x_in = torch.cat([x[edge_index[0]], x[edge_index[1]], e], dim=1)\n",
    "        out = self.edge_network(x_in)\n",
    "        return out\n",
    "\n",
    "    def forward(self, edge_index, x, e):\n",
    "        # propagate: initial call to start propagating messages, takes edge indices and any other data\n",
    "        # propagate calls message, aggreate and update functions\n",
    "        x = self.propagate(edge_index=edge_index, x=x, e=e)\n",
    "        # then we update our edge features, this calls edge_update\n",
    "        e = self.edge_updater(edge_index=edge_index, x=x, e=e)\n",
    "        return x, e\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InteractionNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_iterations=2):\n",
    "        super(InteractionNetwork, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_iterations = n_iterations\n",
    "        \n",
    "        # example network for node encoding; can be replaced with any other network. Experiment here!\n",
    "        self.node_encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # example network for edge encoding; can be replaced with any other network. Experiment here!\n",
    "        self.edge_encoder = nn.Sequential(\n",
    "            nn.Linear(2*hidden_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv = InteractionConv(hidden_dim)\n",
    "        \n",
    "        # example network for edge classification; can be replaced with any other network. Experiment here!\n",
    "        self.edge_classifier = nn.Sequential(\n",
    "            nn.Linear(3* hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "    # we take a full data batch, and make use of features we want \n",
    "    def forward(self, batch):\n",
    "        # Extract node features\n",
    "        x = torch.stack([batch.r, batch.phi, batch.z], dim=-1).to(torch.float32)\n",
    "        edge_index = batch.edge_index\n",
    "        #print(f\"x= {x}\")\n",
    "        #print(f\"edge_index= {edge_index.shape}\")\n",
    "        \n",
    "        #if \"undirected\" in self.hparams and self.hparams[\"undirected\"]:\n",
    "        #edge_index = torch.cat([edge_index, edge_index.flip(0)], dim=1)\n",
    "        #print(f\"edge_index= {edge_index.shape}\")\n",
    "\n",
    "        start, end = edge_index\n",
    "        x.requires_grad = True\n",
    "        #print(start, end)\n",
    "\n",
    "        x = self.node_encoder(x)\n",
    "        e = self.edge_encoder(torch.cat([x[start], x[end]], dim=-1))\n",
    "        #print(x)\n",
    "        #print(e)\n",
    "        # Message passing\n",
    "        for i in range(self.n_iterations):\n",
    "            x, e = self.conv(edge_index, x, e)\n",
    "            \n",
    "        #return\n",
    "        # Decode edge features\n",
    "        decoded = self.edge_classifier(torch.cat([x[start], x[end], e], dim=-1))\n",
    "        \n",
    "        #print(decoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3 # currently assumes r, phi and z in the forward pass \n",
    "# --> (3 input dimensions; you can change this, but you need to adjust the forward pass as well!)\n",
    "hidden_dim = # choose  hidden dimensions: experiment here\n",
    "n_iterations = # choose Number of message passing steps: experiment here\n",
    "\n",
    "model = InteractionNetwork(input_dim=input_dim, hidden_dim=hidden_dim, n_iterations=n_iterations).to(device)\n",
    "print(test_event)\n",
    "test_run = model(test_event.to(device))\n",
    "print(test_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3 # experiment here\n",
    "epochs = 20 # experiment here\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # example optimizer, experiment here\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5) # example scheduler, experiment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def loss_function(output, batch):\n",
    "        \"\"\"\n",
    "        Applies the loss function to the output of the model and the truth labels.\n",
    "        To balance the positive and negative contribution, simply take the means of each separately.\n",
    "        Any further fine tuning to the balance of true target, true background and fake can be handled\n",
    "        with the `weighting` config option.\n",
    "        \"\"\"\n",
    "\n",
    "        assert hasattr(batch, \"y\"), (\n",
    "            \"The batch does not have a truth label. Please ensure the batch has a `y`\"\n",
    "            \" attribute.\"\n",
    "        )\n",
    "        assert hasattr(batch, \"weights\"), (\n",
    "            \"The batch does not have a weighting label. Please ensure the batch\"\n",
    "            \" weighting is handled in preprocessing.\"\n",
    "        )\n",
    "\n",
    "        negative_mask = ((batch.y == 0) & (batch.weights != 0)) | (batch.weights < 0)\n",
    "        #print(output[negative_mask].squeeze().shape)\n",
    "        #print(torch.zeros_like(output[negative_mask]).squeeze().shape)\n",
    "        #print(batch.weights[negative_mask].abs().shape)\n",
    "\n",
    "        negative_loss = F.binary_cross_entropy_with_logits(\n",
    "            output[negative_mask].squeeze(),\n",
    "            torch.zeros_like(output[negative_mask]).squeeze(),\n",
    "            weight=batch.weights[negative_mask].abs(),\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "\n",
    "        positive_mask = (batch.y == 1) & (batch.weights > 0)\n",
    "        positive_loss = F.binary_cross_entropy_with_logits(\n",
    "            output[positive_mask].squeeze(),\n",
    "            torch.ones_like(output[positive_mask]).squeeze(),\n",
    "            weight=batch.weights[positive_mask].abs(),\n",
    "            reduction=\"sum\",\n",
    "        )\n",
    "\n",
    "        n = positive_mask.sum() + negative_mask.sum()\n",
    "        return (\n",
    "            (positive_loss + negative_loss) / n,\n",
    "            positive_loss.detach() / n,\n",
    "            negative_loss.detach() / n,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loops over our optimization code\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    print(f\"current learning rate: {optimizer.param_groups[0]['lr']}\")\n",
    "    for batchid, batch in enumerate(dataloader):\n",
    "\n",
    "        output = model(batch.to(device))\n",
    "        loss, pos_loss, neg_loss = loss_fn(output, batch)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batchid % 10 == 0:\n",
    "            loss, current = loss.item(), batchid * batch_size + len(batch.event_id)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "        # Clear GPU memory\n",
    "        del batch, output, loss\n",
    "        torch.cuda.empty_cache()\n",
    "    scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the model's performance against the test dataset\n",
    "def test_loop(dataloader, model, loss_fn, optimizer, device):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():   \n",
    "        for batchid, batch in enumerate(dataloader):\n",
    "\n",
    "            output = model(batch.to(device))\n",
    "            loss, pos_loss, neg_loss = loss_fn(output, batch)\n",
    "            test_loss += loss\n",
    "\n",
    "            scores = torch.sigmoid(output.squeeze())\n",
    "            #print(scores)\n",
    "            efficiencies = []\n",
    "            purities = []\n",
    "            for i in range(0, 100, 5):\n",
    "                score_cut = i / 100\n",
    "                score_cut_mask = (scores > score_cut)\n",
    "                positive_mask = (batch.y == 1) & (batch.weights > 0)    # only checking for target edges\n",
    "                scored_positive = (score_cut_mask & positive_mask).sum()\n",
    "                #print(f\"scored_negative: {scored_negative}\")\n",
    "                #print(f\"scored_positive: {scored_positive}\")\n",
    "                efficiency = scored_positive / positive_mask.sum()\n",
    "                purity = scored_positive / score_cut_mask.sum()\n",
    "                efficiencies.append(efficiency.item())\n",
    "                purities.append(purity.item())\n",
    "                #print(f\"efficiency: {efficiency}\")\n",
    "                #print(f\"purity: {purity}\")\n",
    "            \n",
    "\n",
    "            # Clear GPU memory\n",
    "            del batch, output, loss, scores, score_cut_mask, positive_mask\n",
    "            torch.cuda.empty_cache()\n",
    "    test_loss /= size\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss.item(), efficiencies, purities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "eff  = []\n",
    "pur  = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_loader, model, loss_function, optimizer, device)\n",
    "    l, e, p = test_loop(val_loader, model, loss_function, optimizer, device)\n",
    "    loss.append(l)\n",
    "    eff.append(e)\n",
    "    pur.append(p)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, param in model.named_parameters():\n",
    "#    print(f\"Parameter name: {name}, Size: {param.size()}, Values: {param}\")\n",
    "\n",
    "# Assuming you have the `test_loss` variable containing the loss values for each epoch\n",
    "import numpy as np\n",
    "\n",
    "epochs_r = range(1, len(loss) + 1)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs_r, loss, 'b', label='Validation loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "score_cut = np.arange(0, 1 ,0.05)\n",
    "print(score_cut)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.xlabel('Score Cut')\n",
    "plt.ylabel('Efficiency')\n",
    "for i, y in enumerate(eff):\n",
    "    if i % 10 == 0 or i == len(pur)-1:\n",
    "        plt.plot(score_cut, y, label=f'Efficiency after epoch {i+1}')\n",
    "#plt.ylim(0.9, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "    \n",
    "score_cut = np.arange(0, 1 ,0.05)\n",
    "print(score_cut)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.xlabel('Score Cut')\n",
    "plt.ylabel('Purity')\n",
    "for i, y in enumerate(pur):\n",
    "    if i % 10 == 0 or i == len(pur)-1:\n",
    "        plt.plot(score_cut, y, label=f'Purity after epoch {pur.index(y)+1}')\n",
    "#plt.ylim(0.5, 1)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# print the final efficiency and purity at 0.05 score cut\n",
    "print(f\"Efficiency: {eff[-1][1]}\")\n",
    "print(f\"Purity: {pur[-1][1]}\")\n",
    "# and at 0.5 score cut --> this would be the ideal operating point\n",
    "print(f\"Efficiency: {eff[-1][10]}\")\n",
    "print(f\"Purity: {pur[-1][10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seb_acorn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
